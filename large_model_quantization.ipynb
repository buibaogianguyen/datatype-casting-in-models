{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46111d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87c6981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "dummy_text = [\"Hello world!\",\n",
    "    \"Testing FP32 vs BF16\",\n",
    "    \"Model quantization\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love machine learning!\",\n",
    "    \"Transformers are amazing for NLP tasks\",\n",
    "    \"This is a very long sentence to test truncation and padding in tokenization\",\n",
    "    \"Short\",\n",
    "    \"Edge case: 1234567890\",\n",
    "    \"Symbols !@#$%^&*()\"]\n",
    "inputs = tokenizer(dummy_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07602046",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bf16 = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3015067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the fp32 model in bytes:  267824136\n",
      "Footprint of the fp32 model in MBs:  267.824136\n"
     ]
    }
   ],
   "source": [
    "fp32_mem_footprint = model.get_memory_footprint()\n",
    "\n",
    "print(\"Footprint of the fp32 model in bytes: \",\n",
    "      fp32_mem_footprint)\n",
    "print(\"Footprint of the fp32 model in MBs: \", \n",
    "      fp32_mem_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3beb0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the bf16 model in MBs:  133.914116\n",
      "Relative diff: 0.5000076468089493\n"
     ]
    }
   ],
   "source": [
    "bf16_mem_footprint = model_bf16.get_memory_footprint()\n",
    "\n",
    "memory_relative_diff = bf16_mem_footprint / fp32_mem_footprint\n",
    "\n",
    "print(\"Footprint of the bf16 model in MBs: \", \n",
    "      bf16_mem_footprint/1e+6)\n",
    "print(f\"Relative diff: {memory_relative_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f68d8",
   "metadata": {},
   "source": [
    "- In contrast to the tiny test model earlier, DistilBERT has many layers, each layer adds up to magnify tiny numeric differences, so a logit in fp32 can be 1.2400, and the logit in bf16 can be 1.2344. This makes comparison by direct output worse for much larger models. Better ways to test model performance after dtype change is by inference/actually using it, relative difference (not memory like above, numerial rel diff), cosine similarity, prediction consistency between classes (%), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90c8c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_fp32 = model(**inputs).logits\n",
    "    out_bf16 = model_bf16(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "326c3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 output:\n",
      "tensor([[ 1.2400, -0.8710],\n",
      "        [-0.1929,  0.0829],\n",
      "        [-0.0969, -0.0873],\n",
      "        [ 0.2087, -0.1834],\n",
      "        [ 0.7831, -0.4950],\n",
      "        [ 0.0362, -0.1076],\n",
      "        [ 0.3799, -0.3108],\n",
      "        [-0.6513,  0.4437],\n",
      "        [ 0.6156, -0.8494],\n",
      "        [ 0.9300, -0.9149]], device='cuda:0')\n",
      "\n",
      "BF16 output:\n",
      "tensor([[ 1.2344, -0.8672],\n",
      "        [-0.2070,  0.0957],\n",
      "        [-0.1064, -0.0776],\n",
      "        [ 0.2139, -0.1875],\n",
      "        [ 0.7812, -0.4902],\n",
      "        [ 0.0510, -0.1191],\n",
      "        [ 0.3711, -0.3027],\n",
      "        [-0.6523,  0.4512],\n",
      "        [ 0.6016, -0.8359],\n",
      "        [ 0.9297, -0.9141]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"FP32 output:\")\n",
    "print(out_fp32)\n",
    "print(\"\\nBF16 output:\")\n",
    "print(out_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7de4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relative numerical difference: 0.015128\n"
     ]
    }
   ],
   "source": [
    "rel_diff = torch.norm(out_fp32 - out_bf16.to(torch.float32)) / torch.norm(out_fp32)\n",
    "print(f\"\\nRelative numerical difference: {rel_diff.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1217cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 predictions: tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "BF16 predictions: tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "Predictions match? True\n"
     ]
    }
   ],
   "source": [
    "pred_fp32 = out_fp32.argmax(dim=-1)\n",
    "pred_bf16 = out_bf16.argmax(dim=-1)\n",
    "\n",
    "print(\"FP32 predictions:\", pred_fp32)\n",
    "print(\"BF16 predictions:\", pred_bf16)\n",
    "print(\"Predictions match?\", (pred_fp32 == pred_bf16).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "236d6266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 1.0009\n"
     ]
    }
   ],
   "source": [
    "cos_sim = torch.nn.functional.cosine_similarity(out_fp32.flatten(), out_bf16.flatten(), dim=0)\n",
    "print(f\"Cosine similarity: {cos_sim.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
